#Restalamos las observaciones que actúan como mediodes
fviz_cluster(object = pam_clusters, data = coches, ellipse.type = "t",
repel = TRUE) +
geom_point(data = medoids, color = "firebrick", size = 2) +
labs(title = "Resultados clustering PAM") +
theme(legend.position = "none")
medoids <- prcomp(coches)$x
medoids <- prcomp(cochesescalados)$x
# Se seleccionan únicamente las proyecciones de las observaciones que son medoids
medoids <- medoids[rownames(pam_clusters$medoids), c("PC1", "PC2")]
medoids <- as.data.frame(medoids)
# Se emplean los mismos nombres que en el objeto ggplot
colnames(medoids) <- c("x", "y")
pam_clusters$medoids
fviz_cluster(object = pam_clusters, data = coches, ellipse.type = "t",
repel = TRUE) +
theme_bw()
#Restalamos las observaciones que actúan como mediodes
fviz_cluster(object = pam_clusters, data = coches, ellipse.type = "t",
repel = TRUE) +
geom_point(data = medoids, color = "firebrick", size = 2) +
labs(title = "Resultados clustering PAM") +
theme(legend.position = "none")
medoids <- prcomp(coches_sin_escalar)$x
medoids <- prcomp(cochesescalados)$x
# Se seleccionan únicamente las proyecciones de las observaciones que son medoids
medoids <- medoids[rownames(pam_clusters$medoids), c("PC1", "PC2")]
medoids <- as.data.frame(medoids)
# Se emplean los mismos nombres que en el objeto ggplot
colnames(medoids) <- c("x", "y")
pam_clusters$medoids
fviz_cluster(object = pam_clusters, data = coches_sin_escalar, ellipse.type = "t",
repel = TRUE) +
theme_bw()
#Restalamos las observaciones que actúan como mediodes
fviz_cluster(object = pam_clusters, data = coches_sin_escalar, ellipse.type = "t",
repel = TRUE) +
geom_point(data = medoids, color = "firebrick", size = 2) +
labs(title = "Resultados clustering PAM") +
theme(legend.position = "none")
coches.eclust = eclust(cochesescalados, FUNcluster = "kmeans", stand = TRUE,
hc_metric = "euclidean", k = 6)
coches.eclust = eclust(cochesescalados, FUNcluster = "pam", stand = TRUE,
hc_metric = "euclidean", k = 6)
medoids <- prcomp(coches_sub_sin_escalar)$x
# Se seleccionan únicamente las proyecciones de las observaciones que son medoids
medoids <- medoids[rownames(pam_clusters$medoids), c("PC1", "PC2")]
medoids <- as.data.frame(medoids)
# Se emplean los mismos nombres que en el objeto ggplot
colnames(medoids) <- c("x", "y")
pam_clusters$medoids
fviz_cluster(object = pam_clusters, data = coches_sub_sin_escalar, ellipse.type = "t",
repel = TRUE) +
theme_bw()
#Restalamos las observaciones que actúan como mediodes
fviz_cluster(object = pam_clusters, data = coches_sub_sin_escalar, ellipse.type = "t",
repel = TRUE) +
geom_point(data = medoids, color = "firebrick", size = 2) +
labs(title = "Resultados clustering PAM") +
theme(legend.position = "none")
#Restalamos las observaciones que actúan como mediodes
fviz_cluster(object = pam_clusters, data = coches, ellipse.type = "t",
repel = TRUE) +
geom_point(data = medoids, color = "firebrick", size = 2) +
labs(title = "Resultados clustering PAM") +
theme(legend.position = "none")
medoids <- prcomp(coches_sub_sin_escalar)$x
# Se seleccionan únicamente las proyecciones de las observaciones que son medoids
medoids <- medoids[rownames(pam_clusters$medoids), c("PC1", "PC2")]
medoids <- as.data.frame(medoids)
# Se emplean los mismos nombres que en el objeto ggplot
colnames(medoids) <- c("x", "y")
pam_clusters$medoids
fviz_cluster(object = pam_clusters, data = coches, ellipse.type = "t",
repel = TRUE) +
theme_bw()
#Restalamos las observaciones que actúan como mediodes
fviz_cluster(object = pam_clusters, data = coches, ellipse.type = "t",
repel = TRUE) +
geom_point(data = medoids, color = "firebrick", size = 2) +
labs(title = "Resultados clustering PAM") +
theme(legend.position = "none")
medoids <- prcomp(coches)$x
medoids <- prcomp(cochesescalados)$x
# Se seleccionan únicamente las proyecciones de las observaciones que son medoids
medoids <- medoids[rownames(pam_clusters$medoids), c("PC1", "PC2")]
medoids <- as.data.frame(medoids)
# Se emplean los mismos nombres que en el objeto ggplot
colnames(medoids) <- c("x", "y")
pam_clusters$medoids
fviz_cluster(object = pam_clusters, data = coches, ellipse.type = "t",
repel = TRUE) +
theme_bw()
#Restalamos las observaciones que actúan como mediodes
fviz_cluster(object = pam_clusters, data = coches, ellipse.type = "t",
repel = TRUE) +
geom_point(data = medoids, color = "firebrick", size = 2) +
labs(title = "Resultados clustering PAM") +
theme(legend.position = "none")
caracteristicas_kmeans$cluster
coches
View(coches)
library(cluster)
library(factoextra)
set.seed(123)
pam_clusters <- pam(x = cochesescalados, k = 6, metric = "euclidean")
pam_clusters$medoids
library(cluster)
library(factoextra)
set.seed(123)
pam_clusters <- pam(x = cochesescalados, k = 6, metric = "manhattan")
pam_clusters$medoids
head(coches,3)
head(coches,50)
coches.eclust = eclust(cochesescalados, FUNcluster = "kmeans", stand = TRUE,
hc_metric = "euclidean", k = 6)
coches.eclust = eclust(cochesescalados, FUNcluster = "pam", stand = TRUE,
hc_metric = "euclidean", k = 6)
coches.eclust = eclust(cochesescalados, FUNcluster = "kmeans", stand = TRUE,
hc_metric = "euclidean", k = 6)
coches.eclust = eclust(cochesescalados, FUNcluster = "pam", stand = TRUE,
hc_metric = "euclidean", k = 6)
coches
coches['opel']
View(coches)
# Cargamos los datos con foreign
library(foreign)
TTerreno = as.data.frame(read.spss("tterreno.sav"))
#Identificación de los NAs por columnas
apply(TTerreno, 2, function(x) {sum(is.na(x))})
#Vemos cuáles son
subset(TTerreno, is.na(peso))
# De esta forma, podemos decidir cómo sustituir; en este caso, por el peso de los otros dos coches equivalentes.
library(tidyverse)
TTerreno$peso=replace_na(TTerreno$peso, 1850)
# con el resto
subset(TTerreno, is.na(cons90))
TTerreno %>%
group_by(marca) %>%
dplyr::summarize(Mean90 = mean(cons90, na.rm=TRUE),
Mean120 = mean(cons120, na.rm=TRUE),
MeanUrb = mean(consurb, na.rm=TRUE))
TTerreno$cons90.2 <- ifelse(TTerreno$marca %in% c("NISSAN") & is.na(TTerreno$cons90), 8.4, TTerreno$cons90)
TTerreno$cons90.3 <- ifelse(TTerreno$marca %in% c("SSANGYONG") & is.na(TTerreno$cons90), 8.17, TTerreno$cons90.2)
# Para los UAZ, por el consumo medio de los TT de 7 plazas
TTerreno %>%
group_by(plazas) %>%
dplyr::summarize(Mean90 = mean(cons90, na.rm=TRUE),
Mean120 = mean(cons120, na.rm=TRUE),
MeanUrb = mean(consurb, na.rm=TRUE))
TTerreno$cons90.4 <- ifelse(TTerreno$marca %in% c("UAZ") & is.na(TTerreno$cons90), 9.29, TTerreno$cons90.3)
#♥ Finalmente, tenemos cons90.4 con todos los consumos y "pisamos" cons90
TTerreno$cons90=TTerreno$cons90.4
# Procedemos igual con los cons120 y consurb:
# ASIA: cons120 de los de 4 plazas
TTerreno$cons120.2 <- ifelse(TTerreno$marca %in% c("ASIA MOTORS") & is.na(TTerreno$cons120), 11, TTerreno$cons120)
# Jeep  Grand Cherokee Jamb por el 2.5TD 3 ptas (justo encima)
TTerreno$cons120.3 <- ifelse(TTerreno$marca %in% c("JEEP") & is.na(TTerreno$cons120), 10.5, TTerreno$cons120.2)
# LADA  por el de los 5 plazas
TTerreno$cons120.4 <- ifelse(TTerreno$marca %in% c("LADA") & is.na(TTerreno$cons120), 12.8, TTerreno$cons120.3)
TTerreno$cons120.5 <- ifelse(TTerreno$marca %in% c("NISSAN") & is.na(TTerreno$cons120), 12.5, TTerreno$cons120.4)
TTerreno$cons120.6 <- ifelse(TTerreno$marca %in% c("SSANGYONG") & is.na(TTerreno$cons120), 12.6, TTerreno$cons120.5)
#  Por último, los UAZ por el consumo medio de los TT de 7 plazas
TTerreno$cons120.7 <- ifelse(TTerreno$marca %in% c("UAZ") & is.na(TTerreno$cons120), 13.5, TTerreno$cons120.6)
TTerreno$cons120=TTerreno$cons120.7
# Eliminamos las sobrantes
TTerreno[,c(16:21)]=NULL
# Actuamos del mismo modo para consurb y velocida
TTerreno$consurb.1 <- ifelse(TTerreno$marca %in% c("JEEP") & is.na(TTerreno$consurb), 9.8, TTerreno$consurb)
TTerreno$consurb.2 <- ifelse(TTerreno$marca %in% c("NISSAN") & is.na(TTerreno$consurb), 12.2, TTerreno$consurb.1)
TTerreno$consurb.3 <- ifelse(TTerreno$marca %in% c("TOYOTA") & is.na(TTerreno$consurb), 10.4, TTerreno$consurb.2) # cambiamos por el análogo - justo encima
TTerreno$consurb=TTerreno$consurb.3
# Eliminamos las sobrantes
TTerreno[,c(16:18)]=NULL
TTerreno$velocida.1 <- ifelse(TTerreno$marca %in% c("SUZUKI") & is.na(TTerreno$velocida), 147, TTerreno$velocida)
TTerreno$velocida.2 <- ifelse(TTerreno$marca %in% c("TATA") & is.na(TTerreno$velocida), 135, TTerreno$velocida.1)
TTerreno$velocida=TTerreno$velocida.2
TT=TTerreno[, c(1:13)]
TT$rpm=NULL
# Comprobamos los NA
apply(TT, 2, function(x) {sum(is.na(x))})
# Uno las dos 1as columnas, y las elimino
TT$TT <- paste(TT$marca,"-",TT$modelo)
TT[,c(1,2)]=NULL
# Como hay duplicados (debido a versiones distintas no recogidas en el nombre del modelo), y eso nos impide renombrar las filas, los re-codificamos
TT$TT <- with(TT, make.unique(as.character(TT)))
TT = data.frame(TT[,-11], row.names=TT[,11])
# Redefinimos las variables cilindros y plazas como numéricas con unfactor de varhandle
library(varhandle)
TT$cilindro=unfactor(TT$cilindro)
TT$plazas=unfactor(TT$plazas)
TT_stats = data.frame(
Min = apply(TT, 2, min), # mín
P25 = apply(TT, 2, quantile, probs=c(0.25), na.rm=TRUE),
Med = apply(TT, 2, median), # mediana
P75 = apply(TT, 2, quantile, probs=c(0.75), na.rm=TRUE),
Max = apply(TT, 2, max), # máx
Mean = apply(TT, 2, mean), # media
SD = apply(TT, 2, sd) # desv est
)
TT_stats = round(TT_stats, 1)
TT_stats
# Tipificamos las variables
TT_tip=scale(TT)
library(factoextra)
library(cluster)
TT.dist = get_dist(TT, stand = TRUE, method = "pearson")
fviz_dist(TT.dist, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"), lab_size = 5)
#podemos visualizar un resumen
fviz_nbclust(nb.todos) + theme_minimal() +
labs(x="Número k de clusters", y="Frecuencia")
# Cargamos los datos con foreign
library(foreign)
TTerreno = as.data.frame(read.spss("tterreno.sav"))
## Tratamiento NAs
#Identificación de los NAs por columnas
apply(TTerreno, 2, function(x) {sum(is.na(x))})
#Vemos cuáles son
subset(TTerreno, is.na(peso))
# De esta forma, podemos decidir cómo sustituir; en este caso, por el peso de los otros dos coches equivalentes.
library(tidyverse)
TTerreno$peso=replace_na(TTerreno$peso, 1850)
# con el resto
subset(TTerreno, is.na(cons90))
# En el caso de los Nissan y Ssanyong sustituiremos con los consumos medios de la marca,
TTerreno %>%
group_by(marca) %>%
dplyr::summarize(Mean90 = mean(cons90, na.rm=TRUE),
Mean120 = mean(cons120, na.rm=TRUE),
MeanUrb = mean(consurb, na.rm=TRUE))
TTerreno$cons90.2 <- ifelse(TTerreno$marca %in% c("NISSAN") & is.na(TTerreno$cons90), 8.4, TTerreno$cons90)
TTerreno$cons90.3 <- ifelse(TTerreno$marca %in% c("SSANGYONG") & is.na(TTerreno$cons90), 8.17, TTerreno$cons90.2)
# Para los UAZ, por el consumo medio de los TT de 7 plazas
TTerreno %>%
group_by(plazas) %>%
dplyr::summarize(Mean90 = mean(cons90, na.rm=TRUE),
Mean120 = mean(cons120, na.rm=TRUE),
MeanUrb = mean(consurb, na.rm=TRUE))
TTerreno$cons90.4 <- ifelse(TTerreno$marca %in% c("UAZ") & is.na(TTerreno$cons90), 9.29, TTerreno$cons90.3)
#♥ Finalmente, tenemos cons90.4 con todos los consumos y "pisamos" cons90
TTerreno$cons90=TTerreno$cons90.4
# Procedemos igual con los cons120 y consurb:
# ASIA: cons120 de los de 4 plazas
TTerreno$cons120.2 <- ifelse(TTerreno$marca %in% c("ASIA MOTORS") & is.na(TTerreno$cons120), 11, TTerreno$cons120)
# Jeep  Grand Cherokee Jamb por el 2.5TD 3 ptas (justo encima)
TTerreno$cons120.3 <- ifelse(TTerreno$marca %in% c("JEEP") & is.na(TTerreno$cons120), 10.5, TTerreno$cons120.2)
# LADA  por el de los 5 plazas
TTerreno$cons120.4 <- ifelse(TTerreno$marca %in% c("LADA") & is.na(TTerreno$cons120), 12.8, TTerreno$cons120.3)
# NISSAN y SSanyong por los consumos medios  de la marca a 120
TTerreno$cons120.5 <- ifelse(TTerreno$marca %in% c("NISSAN") & is.na(TTerreno$cons120), 12.5, TTerreno$cons120.4)
TTerreno$cons120.6 <- ifelse(TTerreno$marca %in% c("SSANGYONG") & is.na(TTerreno$cons120), 12.6, TTerreno$cons120.5)
#  Por último, los UAZ por el consumo medio de los TT de 7 plazas
TTerreno$cons120.7 <- ifelse(TTerreno$marca %in% c("UAZ") & is.na(TTerreno$cons120), 13.5, TTerreno$cons120.6)
##♠ Pisamos cons120 con cons120.7
TTerreno$cons120=TTerreno$cons120.7
# Eliminamos las sobrantes
TTerreno[,c(16:21)]=NULL
# Actuamos del mismo modo para consurb y velocida
TTerreno$consurb.1 <- ifelse(TTerreno$marca %in% c("JEEP") & is.na(TTerreno$consurb), 9.8, TTerreno$consurb)
TTerreno$consurb.2 <- ifelse(TTerreno$marca %in% c("NISSAN") & is.na(TTerreno$consurb), 12.2, TTerreno$consurb.1)
TTerreno$consurb.3 <- ifelse(TTerreno$marca %in% c("TOYOTA") & is.na(TTerreno$consurb), 10.4, TTerreno$consurb.2) # cambiamos por el análogo - justo encima
TTerreno$consurb=TTerreno$consurb.3
# Eliminamos las sobrantes
TTerreno[,c(16:18)]=
# Eliminamos las sobrantes
TTerreno[,c(16:18)]=NULL
# Eliminamos las sobrantes
TTerreno[,c(16:18)]=NULL
TTerreno$velocida.1 <- ifelse(TTerreno$marca %in% c("SUZUKI") & is.na(TTerreno$velocida), 147, TTerreno$velocida)
TTerreno$velocida.2 <- ifelse(TTerreno$marca %in% c("TATA") & is.na(TTerreno$velocida), 135, TTerreno$velocida.1)
TTerreno$velocida=TTerreno$velocida.2
TT=TTerreno[, c(1:13)]
TT$rpm=NULL
# Comprobamos los NA
apply(TT, 2, function(x) {sum(is.na(x))})
# Uno las dos 1as columnas, y las elimino
TT$TT <- paste(TT$marca,"-",TT$modelo)
TT[,c(1,2)]=NULL
# Como hay duplicados (debido a versiones distintas no recogidas en el nombre del modelo), y eso nos impide renombrar las filas, los re-codificamos
TT$TT <- with(TT, make.unique(as.character(TT)))
TT = data.frame(TT[,-11], row.names=TT[,11])
# Redefinimos las variables cilindros y plazas como numéricas con unfactor de varhandle
library(varhandle)
TT$cilindro=unfactor(TT$cilindro)
install.packages('varhandle')
# Redefinimos las variables cilindros y plazas como numéricas con unfactor de varhandle
library(varhandle)
TT$cilindro=unfactor(TT$cilindro)
TT$plazas=unfactor(TT$plazas)
TT_stats = data.frame(
Min = apply(TT, 2, min), # mín
P25 = apply(TT, 2, quantile, probs=c(0.25), na.rm=TRUE),
Med = apply(TT, 2, median), # mediana
P75 = apply(TT, 2, quantile, probs=c(0.75), na.rm=TRUE),
Max = apply(TT, 2, max), # máx
Mean = apply(TT, 2, mean), # media
SD = apply(TT, 2, sd) # desv est
)
TT_stats = round(TT_stats, 1)
TT_stats
# Tipificamos las variables
TT_tip=scale(TT)
library(factoextra)
library(cluster)
TT.dist = get_dist(TT, stand = TRUE, method = "pearson")
fviz_dist(TT.dist, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"), lab_size = 5)
dist.eucl = dist(TT_tip, method = "euclidean", upper=F)
dist.eucl
# Distancia euclídea
library(corrplot)
corrplot(as.matrix(dist.eucl), is.corr = FALSE, method = "color", type="lower", diag=F, order="hclust", tl.cex=0.5, tl.col="dodgerblue4")
#Podemos emplear el dendrograma para visualizar grupos de observaciones similares
plot(hclust(dist.eucl, method = "ward.D2"), cex=0.7, main="Dendrograma", ylab="Anchura",
xlab="Análisis cluster aplicando Ward sobre matriz de distancias euclídeas", cex=0.5)
TT.eclust = eclust(TT, FUNcluster = "kmeans", stand=TRUE, hc_metric="euclidean", nstart=25) # sobre TT, estandarizado con stand=TRUE
TT.eclust$nbclust
## Y pasamos a un jerárquico
TT.eclust.j = eclust(TT, "hclust", k=4) # forzamos a 4 grupos
fviz_dend(TT.eclust.j, rect = TRUE, cex=0.6) # dendrograma con 4 grupos
fviz_silhouette(TT.eclust.j) # silueta
fviz_cluster(TT.eclust.j, pointsize = 2, labelsize = 8, repel=TRUE) # scatter plot
# Conjunto de datos aleatorios
set.seed(123)
n = nrow(TT)
random_df <- data.frame(
x1 = runif(nrow(TT), min(TT$pvp), max(TT$pvp)),
x2 = runif(nrow(TT), min(TT$cilindro), max(TT$cilindro)),
x3 = runif(nrow(TT), min(TT$cc), max(TT$cc)),
x4 = runif(nrow(TT), min(TT$potencia), max(TT$potencia)),
x5 = runif(nrow(TT), min(TT$peso), max(TT$peso)),
x6 = runif(nrow(TT), min(TT$plazas), max(TT$plazas)),
x7 = runif(nrow(TT), min(TT$cons90), max(TT$cons90)),
x8 = runif(nrow(TT), min(TT$cons120), max(TT$cons120)),
x9 = runif(nrow(TT), min(TT$consurb), max(TT$consurb)),
x10 = runif(nrow(TT), min(TT$velocida), max(TT$velocida)))
set.seed(123)
prueba1 = kmeans(TT, 2)
fviz_cluster(list(data = TT, cluster = prueba1$cluster),
ellipse.type = "norm", geom = "point", stand = TRUE)
prueba2 = kmeans(random_df, 2)
fviz_cluster(list(data = random_df, cluster = prueba2$cluster),
ellipse.type = "norm", geom = "point", stand = TRUE)
set.seed(123)
prueba11 = kmeans(TT, 3)
fviz_cluster(list(data = TT, cluster = prueba11$cluster),
ellipse.type = "convex", geom = "point", stand = TRUE)
fviz_cluster(list(data = random_df, cluster = prueba22$cluster),
ellipse.type = "convex", geom = "point", stand = TRUE)
prueba22 = kmeans(random_df, 3)
# Cluster jerárquico sobre el conjunto de datos aleatorios, con k=4
fviz_dend(hclust(dist(random_df)), k = 4,  cex = 0.5)
fviz_dend(hclust(dist(TT)), k = 4,  cex = 0.5)
##Evaluamos la bondad del AC con el método de Hopkins: cuanto más cercano a cero, mejor capacidad de segmentación.
require(clustertend)
# Aplicamos el estadístico sobre los datos reales
set.seed(123)
hopkins(TT_tip, n = nrow(TT)-1)
# y ahora sobre los datos aleatorios
set.seed(123)
hopkins(random_df, n = nrow(random_df)-1)
# Bondad, sobre datos tipificados:
bondad_ac = get_clust_tendency(TT_tip, 100)
# Estadístico de Hopkins
bondad_ac$hopkins_stat
# Gráfico
bondad_ac$plot +
scale_fill_gradient(low = "steelblue", high = "white")
# Con factoextra:
fviz_nbclust(TT_tip, kmeans, method = "wss") +
geom_vline(xintercept = 3, linetype = 2) +
geom_vline(xintercept = 4, linetype = 3) +
ggtitle("Número óptimo de clusters - k medias") +
labs(x="Número k de clusters",y="Suma total de cuadrados intra grupos")
#para jerárquico  --  sugiere 3 grupos
fviz_nbclust(TT_tip,  hcut, method = "wss") +
geom_vline(xintercept = 3, linetype = 2) +
ggtitle("Número óptimo de clusters - jerárquico") +
labs(x="Número k de clusters",y="Suma total de cuadrados intra grupos")
library("NbClust")
set.seed(123)
clus.nb = NbClust(TT_tip, distance = "euclidean",
min.nc = 2, max.nc = 10,
method = "complete", index ="gap")
clus.nb # resultados
# Todos los valores del estadístico de corte
clus.nb$All.index
# Número óptimo de clusters
clus.nb$Best.nc
# Mejor partición
clus.nb$Best.partition
# Cálculo de todos los índices, menos los 4 que son muy exigentes operacionalmente (si los quisiéramos, alllong en vez de all)
nb.todos = NbClust(TT_tip, distance = "euclidean", min.nc = 2,
max.nc = 10, method = "complete", index ="all")
nb.todos
#podemos visualizar un resumen
fviz_nbclust(nb.todos) + theme_minimal() +
labs(x="Número k de clusters", y="Frecuencia")
knitr::opts_chunk$set(echo = TRUE)
library(memisc)
library(haven)
library(foreign)
library(dplyr)
library(factoextra)
library(cluster)
library(factoextra)
require(clustertend)
library("NbClust")
library(FactoMineR)
set.seed(123)
ruta <- 'tterreno.sav'
coches <- read.spss(ruta, to.data.frame = T)
coches <- data.frame(coches[,-1], row.names = make.names(coches[,1], unique = T))
coches_sin_escalar = read.spss(ruta, to.data.frame = T)
coches_sin_escalar <- data.frame(coches_sin_escalar[,-1], row.names = make.names(coches_sin_escalar[,1], unique = T))
coches[116, 11] <- mean(coches[c(119, 120, 118, 121, 117), 11])
coches[116, 11]
coches[c(75:79), 11] <- mean(coches[c(63:74), 11])
coches[19, 11] <- mean(coches[c(13:18, 19:22), 11])
coches[c(105, 106), 12] <- 144
coches[114, 12] <- 135
coches_sin_escalar[116, 11] <- mean(coches_sin_escalar[c(119, 120, 118, 121, 117), 11])
coches_sin_escalar[116, 11]
coches_sin_escalar[c(75:79), 11] <- mean(coches_sin_escalar[c(63:74), 11])
coches_sin_escalar[19, 11] <- mean(coches_sin_escalar[c(13:18, 19:22), 11])
coches_sin_escalar[c(105, 106), 12] <- 144
coches_sin_escalar[114, 12] <- 135
coches_sin_escalar[c(7, 8), 3] <- mean(coches_sin_escalar[c(6, 9:12), 3])
coches_sin_escalar[c(7, 8), 3]
coches_sin_escalar[19, 4] <- mean(coches_sin_escalar[c(13:18, 20:22), 4])
anyNA(coches_sin_escalar)
perfomScaling <-  T
if(perfomScaling){
for(i in names(coches)){
if(class(coches[,i ]) == 'integer' | class(coches[,i ]) == 'numeric'){
coches[,i ] = scale(coches[,i ])
}
}
}
#Creamos un nuevo dataframe, con las columnas buenas.
columnasnum <- c('potencia','rpm','peso','consurb','velocida')
cnum <- c('potencia','rpm','peso','consurb','velocida', 'cons120')
cochesescalados <- subset(coches, select = columnasnum)
cochesescalados2 <- subset(coches, select = cnum)
coches_sub_sin_escalar = subset(coches_sin_escalar, select = columnasnum)
sum(is.na(coches_sub_sin_escalar))
coches_sub_sin_escalar[c(7, 8), 3] = 1850
coches_sub_sin_escalar[19, 4] <- mean(coches_sub_sin_escalar[c(13:18, 20:22), 4])
anyNA(coches_sub_sin_escalar)
#Peso
cochesescalados[c(7, 8), 3] <- mean(cochesescalados[c(6, 9:12), 3])
cochesescalados[c(7, 8), 3]
cochesescalados[19, 4] <- mean(cochesescalados[c(13:18, 20:22), 4])
#Matriz de distancias
#Obtenemos las distancias del anterior DF a través de Pearson
qdist <- get_dist(cochesescalados, stand = T, method = 'pearson')
qdist.manhattan <- get_dist(cochesescalados, stand = T, method = 'manhattan')
qdist.mink <- get_dist(cochesescalados, stand = T, method = 'minkowski')
##Realizamos la representación gráfica.
fviz_dist(qdist, lab_size = 5)
d <- dist(cochesescalados, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward.D2")
plot(fit)
plot(fit, cex = 0.6, hang = -1, main="Dendrograma - hclust")
rect.hclust(fit, k=5, border = 2:4)
#**K-Means Clustering**
k2 <- kmeans(cochesescalados, centers = 2, nstart = 25)
k3 <- kmeans(cochesescalados, centers = 3, nstart = 25)
k4 <- kmeans(cochesescalados, centers = 4, nstart = 25)
k5 <- kmeans(cochesescalados, centers = 5, nstart = 25)
k6 <- kmeans(cochesescalados, centers = 6, nstart = 25)
k7 <- kmeans(cochesescalados, centers = 7, nstart = 25)
k8 <- kmeans(cochesescalados, centers = 8, nstart = 25)
p1 <- fviz_cluster(k2, geom = 'point', data = cochesescalados) + ggtitle('K = 2')
p2 <- fviz_cluster(k3, geom = 'point', data = cochesescalados) + ggtitle('K = 3')
p3 <- fviz_cluster(k4, geom = 'point', data = cochesescalados) + ggtitle('K = 4')
p4 <- fviz_cluster(k5, geom = 'point', data = cochesescalados) + ggtitle('K = 5')
p5 <- fviz_cluster(k6, geom = 'point', data = cochesescalados) + ggtitle('K = 6')
p6 <- fviz_cluster(k7, geom = 'point', data = cochesescalados) + ggtitle('K = 7')
p7 <- fviz_cluster(k8, geom = 'point', data = cochesescalados) + ggtitle('K = 8')
library(gridExtra)
require(ggrepel)
grid.arrange(p1, p2, p3, p4, p5, nrow = 2)
set.seed(123)
caracteristicas_kmeans <- kmeans(cochesescalados, 6)
caracteristicas_kmeans$centers
library(cluster)
library(factoextra)
set.seed(123)
pam_clusters <- pam(x = cochesescalados, k = 6, metric = "manhattan")
pam_clusters$medioids
pam_clusters$clustering
pam_clusters$objective
pam_clusters$isolation
pam_clusters$diss
pam_clusters$call
pam_clusters$id.med
pam_clusters
pam_clusters$silinfo
pam_clusters$silinfo$clus.avg.widths
library(cluster)
library(factoextra)
set.seed(123)
pam_clusters <- pam(x = cochesescalados, k = 6, metric = "manhattan")
pam_clusters$medioids
pam_clusters$medoids
pam_clusters$objective
pam_clusters$diss
pam_clusters$clusinfo
18+16+18+18+36+19
18+16
15*4
18+18+19
